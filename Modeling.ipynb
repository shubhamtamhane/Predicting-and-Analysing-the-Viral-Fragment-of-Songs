{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S7cmp5v-sC1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "from numpy import mean\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Data mining/mfcc_values_0_49.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MlGITuMt-1iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating predictor and target variables\n",
        "\n",
        "X = df[[_ for _ in df.columns if _ not in ['target']]]\n",
        "y = df['target']"
      ],
      "metadata": {
        "id": "O5gBP5h6-31_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dict_values_cv = {'sampling_rate' : [], 'Algorithm_detailed_name' : [], 'Algorithm' : [], 'ROC_AUC_cv':[], 'Accuracy_cv':[], 'F1_cv':[], 'Precision_cv':[], 'Recall_cv': [], 'Accuracy':[], 'F1':[], 'Precision':[], 'Recall':[] }\n",
        "\n",
        "# List of undersampling rates ranging from 0.1 to 1.0\n",
        "sampling_ratios = [round(x, 1) for x in np.arange(0, 1.1, 0.1).tolist()][1:]\n",
        "\n",
        "for rate in sampling_ratios:\n",
        "\n",
        "  # under sampling the majority class\n",
        "  under = RandomUnderSampler(sampling_strategy=rate)\n",
        "\n",
        "  X_under_sampling, y_under_sampling = under.fit_resample(X, y)\n",
        "\n",
        "  # Train Test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_under_sampling, y_under_sampling , test_size = 0.25, random_state = 42)\n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  # define model\n",
        "  model = LogisticRegression(random_state=0)\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(\"Default Logistic Regression\")\n",
        "  dict_values_cv['Algorithm'].append(\"Default Logistic Regression\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "\n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  w = {0:rate, 1:1}\n",
        "\n",
        "  # define model\n",
        "  model = LogisticRegression(random_state=0, class_weight=w)\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(f\"Weighted Logistic Regression {w}\")\n",
        "  dict_values_cv['Algorithm'].append(\"Weighted Logistic Regression\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "\n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  # define model\n",
        "  model = GaussianNB()\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(\"Naive Bayes Classifier\")\n",
        "  dict_values_cv['Algorithm'].append(\"Naive Bayes Classifier\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "\n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  # define model\n",
        "  model = SVC(gamma='scale')\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(\"SVM Classifier\")\n",
        "  dict_values_cv['Algorithm'].append(\"SVM Classifier\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "\n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  # define model\n",
        "  model = SVC(kernel=\"linear\", class_weight=w)\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(f\"Weighted SVM Classifier {w}\")\n",
        "  dict_values_cv['Algorithm'].append(\"Weighted SVM Classifier\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "  \n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  # define model\n",
        "  model = BaggingClassifier()\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(\"Random Forest Classifier\")\n",
        "  dict_values_cv['Algorithm'].append(\"Random Forest Classifier\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "\n",
        "\n",
        "  dict_values_cv['sampling_rate'].append(rate)\n",
        "\n",
        "  # define model\n",
        "  model = BaggingClassifier(base_estimator=SVC(kernel=\"linear\"))\n",
        "  # define evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate model\n",
        "  scores_cv = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "  accuracy_cv = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  f1_cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n",
        "  precision_cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
        "  recall_cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  dict_values_cv['Algorithm_detailed_name'].append(\"SVC Bagging Classifier\")\n",
        "  dict_values_cv['Algorithm'].append(\"SVC Bagging Classifier\")\n",
        "  dict_values_cv['ROC_AUC_cv'].append(mean(scores_cv))\n",
        "  dict_values_cv['Accuracy_cv'].append(mean(accuracy_cv))\n",
        "  dict_values_cv['F1_cv'].append(mean(f1_cv))\n",
        "  dict_values_cv['Precision_cv'].append(mean(precision_cv))\n",
        "  dict_values_cv['Recall_cv'].append(mean(recall_cv))\n",
        "\n",
        "  dict_values_cv['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "  dict_values_cv['F1'].append(f1_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Precision'].append(precision_score(y_test, y_pred, average='binary'))\n",
        "  dict_values_cv['Recall'].append(recall_score(y_test, y_pred, average='binary'))\n",
        "\n"
      ],
      "metadata": {
        "id": "REFkh0Us-6bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting into a DataFrame\n",
        "sampling_comparison_cv = pd.DataFrame(dict_values_cv)\n",
        "sampling_comparison_cv"
      ],
      "metadata": {
        "id": "neCA7b5L-8i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking best models for Undersampling rate < 0.6\n",
        "sampling_comparison_cv[sampling_comparison_cv['sampling_rate']<0.6].sort_values(by = ['Recall_cv','Accuracy_cv', 'sampling_rate'], ascending = [False,False,True])[['sampling_rate',\t'Algorithm_detailed_name',\t'Algorithm','Accuracy_cv', 'Recall_cv', 'Accuracy','Recall']].reset_index(drop = True)"
      ],
      "metadata": {
        "id": "W1mZXzgA--ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validation Accuracy plot"
      ],
      "metadata": {
        "id": "T0yU2tBK_Bnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "figure(figsize=(10, 8), dpi=80)\n",
        "sampling_cv_plot_df = sampling_comparison_cv.copy()\n",
        "\n",
        "sampling_cv_plot_df.set_index('sampling_rate', inplace=True)\n",
        "sampling_cv_plot_df.groupby('Algorithm')['Accuracy_cv'].plot( marker='o', legend=True)\n",
        "plt.title('Cross validation Accuracy comparisons for different algorithms for different UnderSampled data', color='black')\n",
        "plt.ylabel('Accuracy cross validation')\n",
        "plt.xlabel('Undersampling rate')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xZZblngd_B_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validation Recall plot"
      ],
      "metadata": {
        "id": "5yHu6c_5_Fdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "figure(figsize=(10, 8), dpi=80)\n",
        "sampling_cv_plot_df = sampling_comparison_cv.copy()\n",
        "\n",
        "sampling_cv_plot_df.set_index('sampling_rate', inplace=True)\n",
        "sampling_cv_plot_df.groupby('Algorithm')['Recall_cv'].plot(marker='o',legend=True)\n",
        "plt.title('Cross validation Recall comparisons for different algorithms for different UnderSampled data', color='black')\n",
        "plt.ylabel('Recall cross validation')\n",
        "plt.xlabel('Undersampling rate')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WwFWzca8_H3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Accuracy plot"
      ],
      "metadata": {
        "id": "LpAoW0yS_HsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "figure(figsize=(10, 8), dpi=80)\n",
        "sampling_cv_plot_df = sampling_comparison_cv.copy()\n",
        "\n",
        "sampling_cv_plot_df.set_index('sampling_rate', inplace=True)\n",
        "sampling_cv_plot_df.groupby('Algorithm')['Accuracy'].plot( marker='o', legend=True)\n",
        "plt.title('Test Accuracy comparisons for different algorithms for different UnderSampled data', color='black')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.xlabel('Undersampling rate')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHthJBfW_Fp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Recall plot"
      ],
      "metadata": {
        "id": "Y6fgRQGG_Jki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "figure(figsize=(10, 8), dpi=80)\n",
        "sampling_cv_plot_df = sampling_comparison_cv.copy()\n",
        "\n",
        "sampling_cv_plot_df.set_index('sampling_rate', inplace=True)\n",
        "sampling_cv_plot_df.groupby('Algorithm')['Recall'].plot(marker='o',legend=True)\n",
        "plt.title('Test Recall comparisons for different algorithms for different UnderSampled data', color='black')\n",
        "plt.ylabel('Test Recall')\n",
        "plt.xlabel('Undersampling rate')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HaQS-gqx_JwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}